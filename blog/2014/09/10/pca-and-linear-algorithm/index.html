
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>PCA的线性代数的背景 - Wu's Blogs</title>
  <meta name="author" content="Wu Li">

  
  <meta name="description" content="前段时间虽然学了机器学习中关于 PCA 的相关算法，也知道如何利用 PCA 对数据进行降维处理；现有的语言如Python，Octave 都也有很多现成的函数可供利用，非常便捷。但是总是觉得学得囫囵，俗话说：授人以鱼不如授人以渔。还是很期望了解背后的数学原理。 于是钻研了一下，一路上是云漫漫兮白日寒 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://www.wuli.mobi/blog/2014/09/10/pca-and-linear-algorithm">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Wu's Blogs" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Wu's Blogs</a></h1>
  
    <h2>A blog for saving my articles.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:www.wuli.mobi" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">PCA的线性代数的背景</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2014-09-10T09:36:23+08:00'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>10</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>9:36 am</span></time>
        
      </p>
    
  </header>


<div class="entry-content"><p>前段时间虽然学了机器学习中关于 PCA 的相关算法，也知道如何利用 PCA 对数据进行降维处理；现有的语言如Python，Octave 都也有很多现成的函数可供利用，非常便捷。但是总是觉得学得囫囵，俗话说：授人以鱼不如授人以渔。还是很期望了解背后的数学原理。</p>

<p>于是钻研了一下，一路上是云漫漫兮白日寒，天荆地棘行路难。虽说学过高数，念过线性代数，看相关资料还是十分痛苦。实话说本来学得时候就不敢说都懂，现在又忘记的差不多了，这么多年不用，确实够呛。好在网上有一篇对 PCA 数学原理解析得非常好的文章，我估计让我总结也不如这个好，暂且借用一下，放个链接到此。
<a href="http://blog.codinglabs.org/articles/pca-tutorial.html">http://blog.codinglabs.org/articles/pca-tutorial.html</a>
即使有如此好文章，看到后来的矩阵对角化还是感觉要复习的知识太多。对比 PCA 算法的几个步骤和这些算法库给的一些函数，很多情况不明白为什么要这样做，也不明白这是函数的返回值确切数学含义是什么，只是知道如何用而已。</p>

<p>所以，对照上面的这个文章，结合算法，我也来分析这些算法步骤的意义：</p>

<p>PCA 算法
样本数量为m, feature 个数为n</p>

<h2>计算协方差矩阵</h2>

<p>$$
\Sigma=\frac{1}{m}\Sigma_{i=1}^{m}(x^{(i)})(x^{(i)})^{T}
$$
   为什么要算协方差矩阵呢？我们来看看这个算出来的矩阵（我们称这个矩阵为A）包含了些什么东西在里面：</p>

<p>$$
\begin{bmatrix}
\Sigma_{i=1}^{m}x_1^{(i)}x_1^{(i)} &amp;  \Sigma_{i=1}^{m}x_1^{(i)}x_2^{(i)} &amp;  &hellip; &amp; \Sigma_{i=1}^{m}x_1^{(i)}x_n^{(i)} \cr
\Sigma_{i=1}^{m}x_2^{(i)}x_1^{(i)} &amp;  \Sigma_{i=1}^{m}x_2^{(i)}x_2^{(i)} &amp;  &hellip; &amp; \Sigma_{i=1}^{m}x_2^{(i)}x_n^{(i)} \cr
&hellip; &amp; &hellip; &amp; &hellip; &amp; &hellip; \cr
\Sigma_{i=1}^{m}x_n^{(i)}x_1^{(i)} &amp;  \Sigma_{i=1}^{m}x_n^{(i)}x_2^{(i)} &amp;  &hellip; &amp; \Sigma_{i=1}^{m}x_n^{(i)}x_n^{(i)}
\end{bmatrix}=
\begin{bmatrix}
x_1*x_1^{T} &amp;  x_1*x_2^{T} &amp;  &hellip; &amp; x_1*x_n^{T} \cr
x_2*x_1^{T} &amp;  x_2*x_2^{T} &amp;  &hellip; &amp; x_2*x_n^{T} \cr
&hellip; &amp; &hellip; &amp; &hellip; &amp; &hellip; \cr
x_n*x_1^{T} &amp;  x_n*x_2^{T} &amp;  &hellip; &amp; x_n*x_n^{T}
\end{bmatrix}
$$
这个矩阵是什么特点呢？ 这个协方差矩阵是一个对称矩阵。什么？费劲画了这么多知道是对称矩阵又如何？先不用急，先考虑一下 PCA 的目的是什么。PCA 主要目的是降维处理数据，方便各种算法，能够快速处理。在一个样本中，并不是所有的 feature 都同等重要，甚至有时候，有的 feature 根本就没有用（比如 feature $x_2 = 2 * x_1$）, 这个时候，实际上 feature x2的引入并没有实际意义。那么 PCA 正是试图去掉这样的一些 feature，并同时最小化信息丢失。</p>

<p>说得好听？怎么才能“最小化信息的丢失”？这里用到线性变换，全部都是线性代数上的知识。首先，将每个样本看做是一个 n 维向量。想想一下二维空间中得一个向量，经过坐标变换可以得到另外一组坐标；n 维向量也可以通过线性变换得到另外一个向量。如果在变换过程中能够依次找到一个一个的基，在第一个基上，所有样本在其上的投影最长（最能区分不同的样本），第二个基，在垂直于第一个基（内积为0）的所有基的基础上，寻找哪个投影次长的基，第三个基在垂直于前两个基的基础上寻找，以此类推。当我们能够找到 n 个基，在他们上面的投影（l1,l2,l3,&hellip;,ln）即与原始的（x1,x2,x3,&hellip;,xn）相互对应。但是我们的目的是降维，比如从 n 维降低到 k（ 1&lt;=k&lt;n），我们找到的 n 各基，是按照重要程度以此下来的，因此或许找到第 K 的基，在基1,2,&hellip;,K上的线性变换或许就可以满足精度需要。</p>

<p>那么这与协方差矩阵有啥关系呢？观察这个协方差矩阵，假设此时的x1,x2,&hellip;,xn已经是按照我们找到的这一组基变换后的值。那么对角线上，$x_i*x_i^{T}$ (1=&lt;i &lt;=n)  就是方差的一个变异形式，描述了在这个基上面的分散程度，我们希望其能够最大化，这也正是我们刚才找到的那一组基的目的。因此，这两个目标是一致的。</p>

<p>观察上三角区与下三角区，$x_i*x_j$ (1=&lt;i != j &lt;=n) 是不同 feature 之间的协方差。在降维的时候，我们希望选择的各个基之间是线性无关的，也就是互相正交的。因此x1,x2,&hellip;,xn这些在各个基上的投影也应该满足$X_i*X_j=0$,或者说让这个值尽量的趋近于零。
    于是，我们的目标实际上就是对A进行对角化。</p>

<h2>矩阵对角化</h2>

<p>对角化处理在Python中对应的函数有:numpy.linalg.svd，它用来生成特征向量与特征值。
    $$
    U,S,V=np.linalg.svd(A)
    $$</p>

<p>   矩阵对角化在线性代数上是求得特征向量（U）(确切的说，应该是特征向量对应的标准正交基)和特征值（$\lambda$）。数学公式略过，只看定理和结论。
   特征向量与特征值满足如下公式
   A*U=U*$\Lambda$, $\Lambda$是特征值$\lambda$构成的对角矩阵。矩阵 A，这里是我们要进行对角化的矩阵，也是我们最初定义的那个$X*X^{T}$，它最终对角化变成了$\Lambda$，U 是用来进行变化的一组基。S 就是$\lambda_i(1&lt;=1&lt;=n)$</p>

<h2>降维处理</h2>

<p>使用对角化处理得到的U，就可以进行降维处理了，我们的目标既要降维，有不能损失太多的信息，将 S 以降序进行排列。假设我们要从 N 维降到 K 维，我们希望
$$
\frac{\Sigma_{i=1}^{k}S_{ii}}{\Sigma_{i=1}^{n}S_{ii}} >= 0.99
$$</p>

<p>我们可以依次去K=1,2,3,&hellip;,N来使上面的公式得到满足。假设得到这个 K。我们此刻即可对原始的样本 X 进行降维处理
$$
Z=U[0:K,:]*X
$$</p>

<p>此刻 Z 就是我们经过降维化的样本了。</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Wu Li</span></span>

      




<time class='entry-date' datetime='2014-09-10T09:36:23+08:00'><span class='date'><span class='date-month'>Sep</span> <span class='date-day'>10</span><span class='date-suffix'>th</span>, <span class='date-year'>2014</span></span> <span class='time'>9:36 am</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/machine-learning/'>machine learning</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://www.wuli.mobi/blog/2014/09/10/pca-and-linear-algorithm/" data-via="" data-counturl="http://www.wuli.mobi/blog/2014/09/10/pca-and-linear-algorithm/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2014/08/29/latex-support/" title="Previous Post: Latex support on github blogs">&laquo; Latex support on github blogs</a>
      
      
        <a class="basic-alignment right" href="/blog/2014/11/05/covariance/" title="Next Post: 期望值，方差和协方差（3）">期望值，方差和协方差（3） &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/01/15/2016-01-15-pythonexp2/">Python 语言使用回顾 (二)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/09/pythonexp/">Python 语言使用回顾</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/08/24/resume/">Resume</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/05/04/closures/">闭包</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/04/30/legb/">LEGB规则与 Python 变量的存取</a>
      </li>
    
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - Wu Li -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'httpwulimobi';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://www.wuli.mobi/blog/2014/09/10/pca-and-linear-algorithm/';
        var disqus_url = 'http://www.wuli.mobi/blog/2014/09/10/pca-and-linear-algorithm/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
