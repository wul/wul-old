<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Math | Wu's Blogs]]></title>
  <link href="http://www.wuli.mobi/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://www.wuli.mobi/"/>
  <updated>2017-06-04T10:50:33+08:00</updated>
  <id>http://www.wuli.mobi/</id>
  <author>
    <name><![CDATA[Wu Li]]></name>
    <email><![CDATA[li.koun@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[向量的正交性]]></title>
    <link href="http://www.wuli.mobi/blog/2014/11/17/orthogonal/"/>
    <updated>2014-11-17T09:24:43+08:00</updated>
    <id>http://www.wuli.mobi/blog/2014/11/17/orthogonal</id>
    <content type="html"><![CDATA[<h1>正交性 (Orthogonal)</h1>

<p>在向量空间中，如何评价两个向量之间的角度，或者更加进一步说他们之间的相关程度？ 想想在一个二维空间中，一个通过原点的向量的长度和方向由其坐标（x,y）决定。如果同时存在另外一个类似的向量，他们之间会存在一个夹角。这个夹角要么大于或者小于90度，要么正好90度。</p>

<p>这两个向量之间的夹角符合下面公式:</p>

<p>$$cos(\theta)=\frac{x^{T}y}{||x||*||y||}$$</p>

<p>这个公式可以由<strong>余弦定理</strong>推导得出。在此不推导了。我们关系的是两个向量的相关程度。如果2个向量垂直，我们说他们无关，或说正交。如果两个向量方向相同，则相关度最大。由上面这个公式可以看出，如果$\theta$为90度的时候，即，x，y 两个向量的内积为零。 如果两个向量之间夹角为0，则有</p>

<p>$$ ||x|| * ||y|| = x^{T} y $$</p>

<p>即内积等于向量长度之积。（如果向量方向相反，则等于内积的负值）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[向量空间]]></title>
    <link href="http://www.wuli.mobi/blog/2014/11/10/vector/"/>
    <updated>2014-11-10T09:45:49+08:00</updated>
    <id>http://www.wuli.mobi/blog/2014/11/10/vector</id>
    <content type="html"><![CDATA[<h1>向量空间的一些相关知识</h1>

<p>关于向量空间 V 的公理：V 定义了加法和标量乘法运算的集合。对于 V 中每一对元素 x 和 y，x+y 与 V 中一个元素相对应；同时如果有一个标量$\alpha$，有$\alpha$x 也与 V 中一个元素相对应。集合 V 称为向量空间（Vector Space）的前提是：除过这两个，并满足下面公理</p>

<ul>
<li>V 中的任意元素 x，y 有 x+y=y+x</li>
<li>V 中的任意元素 x，y，z，有 (x+y)+z = x+(y+z)</li>
<li>V 中存在元素0, 对于任何元素 $x\in_{}V$，有 x+0=x</li>
<li>对于$x\in_{}V$，有 x+(-x)=0</li>
<li>a(x+y)=ax+ay</li>
<li>(a+b)x=ax+bx</li>
<li>(ab)x=a(bx)</li>
<li>1*x=x</li>
</ul>


<p>这些运算称作线性运算。向量空间的定义很冗长，但是是基础。学不好基础，就像我这样，大学毕业很久，看一些书，还得回来重新复习。<strong>向量空间有个很重要的属性，就是其封闭性</strong>，任何向量经过加法或者与标量的乘法运算，或者同时进行这两种运算，都对于向量空间中得某一个向量。</p>

<p>向量空间又称作线性空间。</p>

<h1>子空间</h1>

<p>若 S 为向量空间 V 的非空子集，且 S 满足如下条件：</p>

<ul>
<li>对已任意标量$\alpha$，若 $x\in{S}$，则$\alpha{x}\in{S}$</li>
<li>若$x\in{S}$ 和 $y\in{S}$，则$x+y\in{S}$</li>
</ul>


<p>定义 S 为 V 的子空间（subspace）。这两个条件表明子空间在加法和乘法意义下，如同向量空间一样，也是<strong>封闭</strong>的。</p>

<p>话都明白，但是不大容易理解子空间到底是什么。举个栗子，V 为一三维空间，则在此三维空间中，我们找到一个平面，则此平面上的所有向量构成 V 的子空间 S。此子空间同时也满足封闭性；因为此子空间也满足向量空间所满足的八个公理，于是向量空间的子空间仍然是向量空间。</p>

<p>集合{0}为零子空间。</p>

<h1>向量的张成（span）的定义</h1>

<p>若$v_1,v_2,&hellip;,v_n$是向量空间 V 中的向量，则$\alpha_1v_1+\alpha_2v_2+&hellip;+\alpha_nv_n$（$\alpha$为标量）称为向量$v_1,v_2,&hellip;,v_n$的线性组合，所有线性组合构成的集合成为$v_1,v_2,&hellip;v_n$的张成，记作Span($v_1,v_2,&hellip;,v_n$)。</p>

<h1>向量空间的张集（spanning set）的定义</h1>

<p>有一种张集的特殊情况，那就是 Span($v_1,v_2,&hellip;,v_n$)=V。于是定义：
($v_1,v_2,&hellip;,v_n$)是向量空间 V 的一个张集的充要条件是 V 中的任何一个向量都一个表示为($v_1,v_2,&hellip;,v_n$)的线性组合。</p>

<h1>线性无关（linear independence）</h1>

<p>寻找最小张集的问题。</p>

<p>定义：如果向量空间 V 中的 ($v_1,v_2,&hellip;,v_n$) 满足 ($c_1v_1,c_2v_2,&hellip;,c_nv_n=0$) 只有在$c_1,c_2,&hellip;,c_n$全部为0的情况下才满足，则它们是线性无关的。</p>

<p>于是寻找最小张集的问题就是寻找这么一组想想，($v_1,v_2,&hellip;,v_n$)，使得它们线性无关。最小张集又称作基（basis）。</p>

<p>线性相关的定义则是能够找到不全为0的$c_1,c_2,&hellip;,c_n$，使得</p>

<p>$$c_1v_1+c_2v_2+&hellip;+c_nv_n=0$$</p>

<p>结合向量</p>

<h1>基</h1>

<p>首先基是一组线性无关的向量；并且这组向量张成向量空间 V。这就是基的定义。</p>

<p>如果向量空间 V 的维数为 n>0,则
+ 任意 n 个线性无关的向量可以张成 V
+ 任何张成 V 的 n 各向量都线性无关</p>

<p>标准基 {$e_1,e_2,&hellip;,e_n$}.</p>

<h1>向量空间 V 的维数</h1>

<p>定义：V 的一组基含有的 n 个向量，称之为 V 的维数。</p>

<h1>坐标变换</h1>

<p>标准基[$e_1,e_2,&hellip;,e_n$]下的一个向量X ($x_1,x_2,&hellip;,x_n$)在新基[$u_1,u_2,&hellip;,u_n$]的坐标向量是</p>

<p>$$c=U^{-1}x$$</p>

<p>反之，如果一个向量在 U 基的坐标向量是 C，那么对应原标准基的坐标为</p>

<p>$$x=Uc$$</p>

<p>U称作从有序基 U 到基 E 的转移矩阵（transition matrix）</p>

<h1>行空间和列空间</h1>

<p>矩阵 A 的行空间的维数称为矩阵 A 的秩（rank）。 怎么理解？ 维数是基的个数，也就是最大线性无关向量组的向量个数。因此:</p>

<p>Rank(A)=Number of {$e_1,e_2,&hellip;,e_n$}</p>

<h1>线性映射与线性变换</h1>

<p>线性映射：从空间 A 到 B 的映射，又成线性变换。 (Linear map/linear mapping/linear transformation, or linear function under some scenarios)
$$y = \tau(x)$$</p>

<p>$$x\in{A};y\in{B}$$</p>

<p>同时需要满足$\tau(x+y)=\tau(x)+\tau(y)$和$\tau(\alpha{x})=\alpha\tau(x)$
线性映射的像空间如果是零空间，那么使得$\tau(\alpha)=0$的$\alpha$的全体向量本身也是一个线性空间，称作线性变换$\tau$的核。</p>

<p>$S_\tau = {\alpha|\alpha\in{V},\tau(\alpha)=0}$</p>

<p>线性算子 (Linear Operator，或者叫自同态，endomorphism)：从空间 A 到自身的映射</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[期望值，方差和协方差（2）]]></title>
    <link href="http://www.wuli.mobi/blog/2014/11/05/variance/"/>
    <updated>2014-11-05T08:05:09+08:00</updated>
    <id>http://www.wuli.mobi/blog/2014/11/05/variance</id>
    <content type="html"><![CDATA[<p>上篇说了期望值，我们知道这是一种对结果的预期。有了这个基础，我们可以进一步讨论下一个概念：方差（Variance）</p>

<h1>方差 （ Variance）</h1>

<p>方差就是平法差。它的数学定义是:</p>

<p>$$Var(X)=\sum_{s\in_S}(X(s)-\mu)^{2}P(s)   \qquad 离散随机变量$$</p>

<p>$\mu$就是期望值E[x]。方差用Var(X)或者$\sigma^{2}$来表示。这个公式是说，对于每个样本，首先计算其随机变量值与期望值差异的平方，然后乘以出现这个样本的概率，最后对于所有样本做一个总和。从公式的形式来看，我们如果把$(X(s)-\mu)^{2}$看做s的随机变量的话，其实方差也算是一个新的随机变量的期望值。这是另外一个角度看待方差，于是方差亦可以表示为:</p>

<p>$$Var(X)=E[(x-\mu)^{2}]$$</p>

<p>在这里，方差就是对于结果偏差（的平方）的一个预期。学术点的说法就是描述离散程度的一个量。</p>

<p>这个公式经过推导（利用期望值的运算公式）可以得到：</p>

<p>$$Var(X) = E[X^{2}] - E[X]^{2}$$</p>

<p>一些方差的运算公式有</p>

<p>$$
\begin{align*}
Var(X+a) &amp;= Var(X) &#92;
Var(aX) &amp;= a^{2}Var(X)&#92;
Var(aX+bY) &amp;= a^{2}Var(X)+2abCov(X,Y)+b^{2}Var(Y)&#92;
\end{align*}
$$</p>

<p>第三个等式的更一般形式为：</p>

<p>$$Var(\sum_{i=1}^N a_iX_i)=\sum_{i,j=1}^NCov(X_i,X_j)=\sum_{i=1}^Na_i^{2}Var(X_i)+2\sum_{1\leq{i}\leq{j}\leq{N}}^Na_ia_jCov(X_iX_j)$$</p>

<p>至于Cov，就是协方差（Covariance）的缩写。下次再说。</p>

<p>这里的特例是如果 X 和 Y 为独立变量，则有</p>

<p>$$Var(X+Y) &amp;= Var(X)+Var(Y) \qquad X,Y 为独立变量 $$</p>

<p>这个可以由期望值的定义和运算公式推出。</p>

<p>总而言之，方差就是描述数值的离散程度。离散程度越高，方差越大。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[期望值，方差和协方差（1）]]></title>
    <link href="http://www.wuli.mobi/blog/2014/11/05/expected-value/"/>
    <updated>2014-11-05T08:05:09+08:00</updated>
    <id>http://www.wuli.mobi/blog/2014/11/05/expected-value</id>
    <content type="html"><![CDATA[<p>学习统计学、机器学习甚至线性代数的时候，总是有几个概念时不时跳出来，伴随着整个学习过程。起初的时候，你好像知道他们是干什么用的，慢慢地，你得回头过去看看到底他们是做什么用得，甚至还要思考，他们为什么还会存在。为什么数学家要将他们定义成这个样子，而不是其他的形式。他们就是期望值、方差、协方差，许多讨论概率的分布，随机变量的相关性等问题，都与他们相联系。因此，真正搞懂他们的意义，本身的意义就很重大。</p>

<h1>期望值 (Expected Value/Expectation)</h1>

<p>期望值的数学定义为：
$$E[X]=\sum_{s\in_S}X(s)P(s)   \qquad 离散随机变量$$</p>

<p>其中 X 是<strong>随机变量</strong>，s是<strong>样本</strong>，S 是<strong>样本空间</strong>，P(s)是样本的<strong>概率</strong>。每个样本的概率可以看做是一个权重，于是在这个样本下对应的随机变量的值与这个权重的乘积描述了此随机变量值对总体期望的一个“贡献值”。将所有这些贡献值相加，得到对总体随机变量值的一个期望。那么这个期望又有什么意思呢？通过一个例子来理解这个期望比较容易，比如说掷色子，色子有6个面，一个理想没有作弊过的色子我们认为（注意，是认为，为什么这么认为呢？以后再说）在其被掷出，每个面朝上的几率都是1/6，样本空间为{1，2，3，4，5，6}（集合中的每个值代表每个面的点数）。将这些信息带入上面的期望值中得定义里可得
 E[X]=1<em>1/6+2</em>1/6+&hellip;+6*1/6=3.5</p>

<p>这个3.5与任意一次掷色子的结果{1，2，3，4，5，6}都不同，但它描述了一个趋势，那就是，如果我们掷色子足够多次数，我们得到的所有结果，其点数的平均值就是3.5。或许你连着6次都扔了6点，但是当扔足够大的次数时候，所有点数的平均值比如等于3.5（大数定理）。</p>

<p>听起来期望值似乎与平均值一个意思，单单从数值上来讲，对于足够多次的实验结果，这两个值是一样的。但是这两个概念的定义是不同的。期望值对应的是所有<strong>样本</strong>，即掷色子出现{1，2，3，4，5，6}这个样本空间；而平均值是针对投掷色子的<strong>结果</strong>，如{2，3，5，6，6，6，2，3，&hellip;}。</p>

<p>上面关于期望值的定义可以推导出另外一种形式的定义：</p>

<p>$$E[X]=\sum_{r}(X=r)P(X=r)   \qquad 离散随机变量$$</p>

<p>这个形式描述了随机变量的值与这个值所出现的概率。对于连续随机变量，期望值的定义如下：</p>

<p>$$E[X]=\int^{+\infty}_{-\infty}xf(x)dx   \qquad 连续随机变量$$</p>

<p>这里f(x)是概率密度函数。</p>

<h1>这就是期望值？没啥花头么</h1>

<p>方差就是这样定义，似乎描述了一个类似平均或者重心的概念，没有什么啥好想的。其实不然，我们从一个小赌博开始，假设你和另外一个人投硬币游戏，那个人对你说，我们玩个游戏，连续猜五次正反面为一局，如果都猜对，给你30圆，如果输，你给他一元。 你说你赌不赌？赌是可以赌的，小赌怡情么，不过对于当前这个规则，假设你们能够赌到天荒地老，时间管够。你可是必输无疑的。为什么呢？我们算算期望值，也就是你的理想收益（足够多局数情况下）。</p>

<p>对于连着猜5次都能猜正确，我们知道，这个概率是$(&frac12;)^{5} = 1/32$
于是你有1/32的概率得到30圆，而又31/32的概率输掉1圆钱。那么期望值E(X)=1/32*30+31/32*(-1)=-1/32圆。这意味着你玩这个游戏，每次的收益期望是-1/32圆。那么时间玩得够久，你就损失大了。 当然，话说回来，这也不排除你第一把就全猜对5个正反面，赢了30 圆跑路。</p>

<p>概率论本身就是研究这个世界不确定性，它的发展反映了人性的一些特点，历史上很多人把概率应用到各种赌博中去，甚至有人通过计算，发了横财。这再次说明，知识就是生产力啊。</p>

<p>总体而言，期望着是给你一个预期，让你理性判别得失。</p>

<p>作为结束，提一下期望值的一些运算规则，很好证明，不再赘述。</p>

<p>$$
\begin{align*}
E[X+b] &amp;= E[X] + b &#92;
E[X+Y] &amp;= E[X] + E[Y] &#92;
E[aX] &amp;= aE[X] &#92;
E[XY] &amp;= E[X]E[Y] \qquad X,Y是独立变量&#92;
\end{align*}
$$</p>

<p>关于方差 (Variance) ，且听下回分解。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[期望值，方差和协方差（3）]]></title>
    <link href="http://www.wuli.mobi/blog/2014/11/05/covariance/"/>
    <updated>2014-11-05T08:05:09+08:00</updated>
    <id>http://www.wuli.mobi/blog/2014/11/05/covariance</id>
    <content type="html"><![CDATA[<p>在多随机变量的方差运算中，我们看到了协方差的身影 COV。协方差的定义为：</p>

<p>$$cov(X,Y)=E[(X-\mu)(Y-\nu)] = E[XY]-E[X]E[Y]$$</p>

<p>$\mu$是 X 的期望值 E[X]，$\nu$是 Y 的期望值 E[Y]。协方差是描述两个随机变量之间的。协方差有时候也用符号$\sigma$来表示。这个公式看起来有点眼熟，如果我们让 Y=X，实际上它就变成了方差的定义。这不是偶合，下面详细说明。</p>

<p>协方差表示的是两个随机变量的总体误差，并描述两个随机变量之间的线性相关度。为了方便，我们经常使用规范化（normalized）的协方差。</p>

<p>$$corr(X,Y)=\frac{cov(X,Y)}{\sqrt{Var(X)Var(Y)}} $$</p>

<p>由柯西不等式可以知道这个值介于-1与1之间。 下图第一排反映的是2个随机变量（$X_i,Y_i$）构成的点集的相关性关系。如果两个随机变量正相关，即Y和 X 之间符合Y=aX+b，则它们的构成的点集为一条直线（corr=1）；如果2个随机变量相反，但符合 Y=-aX+b，此时他们负相关相反（corr=-1）；同时，还有一个特点，如果2个随机变量独立，或者说它们是不相关的，此时，则构成的点集是一团（corr=0）。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Correlation_examples.png/800px-Correlation_examples.png" alt="aaaa" /></p>

<center>图片取自wikipedia</center>


<p>由方差公式 Var(X+Y)=Var(X)+Var(Y)+Cov(X,Y)知道，如果 X，Y 为独立变量，则 Cov(X,Y)=0。但不能翻过来说。亦，如果 Cov(X,Y)=0， 则 X,Y 互为独立变量是错误的。比如在[-1,1]之间 $Cov(X,X<sup>2</sup>)$为0（根据 Cov 定义，代$入 X，X<sup>2</sup>积分可得），但不能说 X 和 X<sup>2</sup>$ 是独立的。</p>

<p>另一方面，如果两个随机变量非常相关，比如 Y=X，那么相关度公式</p>

<p>$$corr(X,X)=\frac{Var(X)}{\sqrt{Var(X)<sup>2</sup>}}=1$$</p>

<p>从这个特例可直觉上印证随机变量的相关度。</p>

<p>在线性代数中，对于一个矩阵 A，其由 N 个列向量组成，每个列向量可以看做是一个随机变量的不同值构成，那么它的协方差矩阵为</p>

<p>$$\Sigma=E[(X-E[X])^{T}(X-E[X])]$$</p>

<p>协方差矩阵在这里描述的是任意两个向量之间的线性相关度（一般将 E(x)归一化成0）。在机器学习中得降维处理中有所应用。其目的在于寻找一组基使得排除一些不重要的维度成为可能。</p>
]]></content>
  </entry>
  
</feed>
