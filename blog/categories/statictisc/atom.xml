<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Statictisc | Wu's Blogs]]></title>
  <link href="http://www.wuli.mobi/blog/categories/statictisc/atom.xml" rel="self"/>
  <link href="http://www.wuli.mobi/"/>
  <updated>2017-06-02T22:54:38+08:00</updated>
  <id>http://www.wuli.mobi/</id>
  <author>
    <name><![CDATA[Wu Li]]></name>
    <email><![CDATA[li.koun@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[期望值，方差和协方差（2）]]></title>
    <link href="http://www.wuli.mobi/blog/2014/11/05/variance/"/>
    <updated>2014-11-05T08:05:09+08:00</updated>
    <id>http://www.wuli.mobi/blog/2014/11/05/variance</id>
    <content type="html"><![CDATA[<p>上篇说了期望值，我们知道这是一种对结果的预期。有了这个基础，我们可以进一步讨论下一个概念：方差（Variance）</p>

<h1>方差 （ Variance）</h1>

<p>方差就是平法差。它的数学定义是:</p>

<p>$$Var(X)=\sum_{s\in_S}(X(s)-\mu)^{2}P(s)   \qquad 离散随机变量$$</p>

<p>$\mu$就是期望值E[x]。方差用Var(X)或者$\sigma^{2}$来表示。这个公式是说，对于每个样本，首先计算其随机变量值与期望值差异的平方，然后乘以出现这个样本的概率，最后对于所有样本做一个总和。从公式的形式来看，我们如果把$(X(s)-\mu)^{2}$看做s的随机变量的话，其实方差也算是一个新的随机变量的期望值。这是另外一个角度看待方差，于是方差亦可以表示为:</p>

<p>$$Var(X)=E[(x-\mu)^{2}]$$</p>

<p>在这里，方差就是对于结果偏差（的平方）的一个预期。学术点的说法就是描述离散程度的一个量。</p>

<p>这个公式经过推导（利用期望值的运算公式）可以得到：</p>

<p>$$Var(X) = E[X^{2}] - E[X]^{2}$$</p>

<p>一些方差的运算公式有</p>

<p>$$
\begin{align*}
Var(X+a) &amp;= Var(X) &#92;
Var(aX) &amp;= a^{2}Var(X)&#92;
Var(aX+bY) &amp;= a^{2}Var(X)+2abCov(X,Y)+b^{2}Var(Y)&#92;
\end{align*}
$$</p>

<p>第三个等式的更一般形式为：</p>

<p>$$Var(\sum_{i=1}^N a_iX_i)=\sum_{i,j=1}^NCov(X_i,X_j)=\sum_{i=1}^Na_i^{2}Var(X_i)+2\sum_{1\leq{i}\leq{j}\leq{N}}^Na_ia_jCov(X_iX_j)$$</p>

<p>至于Cov，就是协方差（Covariance）的缩写。下次再说。</p>

<p>这里的特例是如果 X 和 Y 为独立变量，则有</p>

<p>$$Var(X+Y) &amp;= Var(X)+Var(Y) \qquad X,Y 为独立变量 $$</p>

<p>这个可以由期望值的定义和运算公式推出。</p>

<p>总而言之，方差就是描述数值的离散程度。离散程度越高，方差越大。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[期望值，方差和协方差（1）]]></title>
    <link href="http://www.wuli.mobi/blog/2014/11/05/expected-value/"/>
    <updated>2014-11-05T08:05:09+08:00</updated>
    <id>http://www.wuli.mobi/blog/2014/11/05/expected-value</id>
    <content type="html"><![CDATA[<p>学习统计学、机器学习甚至线性代数的时候，总是有几个概念时不时跳出来，伴随着整个学习过程。起初的时候，你好像知道他们是干什么用的，慢慢地，你得回头过去看看到底他们是做什么用得，甚至还要思考，他们为什么还会存在。为什么数学家要将他们定义成这个样子，而不是其他的形式。他们就是期望值、方差、协方差，许多讨论概率的分布，随机变量的相关性等问题，都与他们相联系。因此，真正搞懂他们的意义，本身的意义就很重大。</p>

<h1>期望值 (Expected Value/Expectation)</h1>

<p>期望值的数学定义为：
$$E[X]=\sum_{s\in_S}X(s)P(s)   \qquad 离散随机变量$$</p>

<p>其中 X 是<strong>随机变量</strong>，s是<strong>样本</strong>，S 是<strong>样本空间</strong>，P(s)是样本的<strong>概率</strong>。每个样本的概率可以看做是一个权重，于是在这个样本下对应的随机变量的值与这个权重的乘积描述了此随机变量值对总体期望的一个“贡献值”。将所有这些贡献值相加，得到对总体随机变量值的一个期望。那么这个期望又有什么意思呢？通过一个例子来理解这个期望比较容易，比如说掷色子，色子有6个面，一个理想没有作弊过的色子我们认为（注意，是认为，为什么这么认为呢？以后再说）在其被掷出，每个面朝上的几率都是1/6，样本空间为{1，2，3，4，5，6}（集合中的每个值代表每个面的点数）。将这些信息带入上面的期望值中得定义里可得
 E[X]=1<em>1/6+2</em>1/6+&hellip;+6*1/6=3.5</p>

<p>这个3.5与任意一次掷色子的结果{1，2，3，4，5，6}都不同，但它描述了一个趋势，那就是，如果我们掷色子足够多次数，我们得到的所有结果，其点数的平均值就是3.5。或许你连着6次都扔了6点，但是当扔足够大的次数时候，所有点数的平均值比如等于3.5（大数定理）。</p>

<p>听起来期望值似乎与平均值一个意思，单单从数值上来讲，对于足够多次的实验结果，这两个值是一样的。但是这两个概念的定义是不同的。期望值对应的是所有<strong>样本</strong>，即掷色子出现{1，2，3，4，5，6}这个样本空间；而平均值是针对投掷色子的<strong>结果</strong>，如{2，3，5，6，6，6，2，3，&hellip;}。</p>

<p>上面关于期望值的定义可以推导出另外一种形式的定义：</p>

<p>$$E[X]=\sum_{r}(X=r)P(X=r)   \qquad 离散随机变量$$</p>

<p>这个形式描述了随机变量的值与这个值所出现的概率。对于连续随机变量，期望值的定义如下：</p>

<p>$$E[X]=\int^{+\infty}_{-\infty}xf(x)dx   \qquad 连续随机变量$$</p>

<p>这里f(x)是概率密度函数。</p>

<h1>这就是期望值？没啥花头么</h1>

<p>方差就是这样定义，似乎描述了一个类似平均或者重心的概念，没有什么啥好想的。其实不然，我们从一个小赌博开始，假设你和另外一个人投硬币游戏，那个人对你说，我们玩个游戏，连续猜五次正反面为一局，如果都猜对，给你30圆，如果输，你给他一元。 你说你赌不赌？赌是可以赌的，小赌怡情么，不过对于当前这个规则，假设你们能够赌到天荒地老，时间管够。你可是必输无疑的。为什么呢？我们算算期望值，也就是你的理想收益（足够多局数情况下）。</p>

<p>对于连着猜5次都能猜正确，我们知道，这个概率是$(&frac12;)^{5} = 1/32$
于是你有1/32的概率得到30圆，而又31/32的概率输掉1圆钱。那么期望值E(X)=1/32*30+31/32*(-1)=-1/32圆。这意味着你玩这个游戏，每次的收益期望是-1/32圆。那么时间玩得够久，你就损失大了。 当然，话说回来，这也不排除你第一把就全猜对5个正反面，赢了30 圆跑路。</p>

<p>概率论本身就是研究这个世界不确定性，它的发展反映了人性的一些特点，历史上很多人把概率应用到各种赌博中去，甚至有人通过计算，发了横财。这再次说明，知识就是生产力啊。</p>

<p>总体而言，期望着是给你一个预期，让你理性判别得失。</p>

<p>作为结束，提一下期望值的一些运算规则，很好证明，不再赘述。</p>

<p>$$
\begin{align*}
E[X+b] &amp;= E[X] + b &#92;
E[X+Y] &amp;= E[X] + E[Y] &#92;
E[aX] &amp;= aE[X] &#92;
E[XY] &amp;= E[X]E[Y] \qquad X,Y是独立变量&#92;
\end{align*}
$$</p>

<p>关于方差 (Variance) ，且听下回分解。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[期望值，方差和协方差（3）]]></title>
    <link href="http://www.wuli.mobi/blog/2014/11/05/covariance/"/>
    <updated>2014-11-05T08:05:09+08:00</updated>
    <id>http://www.wuli.mobi/blog/2014/11/05/covariance</id>
    <content type="html"><![CDATA[<p>在多随机变量的方差运算中，我们看到了协方差的身影 COV。协方差的定义为：</p>

<p>$$cov(X,Y)=E[(X-\mu)(Y-\nu)] = E[XY]-E[X]E[Y]$$</p>

<p>$\mu$是 X 的期望值 E[X]，$\nu$是 Y 的期望值 E[Y]。协方差是描述两个随机变量之间的。协方差有时候也用符号$\sigma$来表示。这个公式看起来有点眼熟，如果我们让 Y=X，实际上它就变成了方差的定义。这不是偶合，下面详细说明。</p>

<p>协方差表示的是两个随机变量的总体误差，并描述两个随机变量之间的线性相关度。为了方便，我们经常使用规范化（normalized）的协方差。</p>

<p>$$corr(X,Y)=\frac{cov(X,Y)}{\sqrt{Var(X)Var(Y)}} $$</p>

<p>由柯西不等式可以知道这个值介于-1与1之间。 下图第一排反映的是2个随机变量（$X_i,Y_i$）构成的点集的相关性关系。如果两个随机变量正相关，即Y和 X 之间符合Y=aX+b，则它们的构成的点集为一条直线（corr=1）；如果2个随机变量相反，但符合 Y=-aX+b，此时他们负相关相反（corr=-1）；同时，还有一个特点，如果2个随机变量独立，或者说它们是不相关的，此时，则构成的点集是一团（corr=0）。</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Correlation_examples.png/800px-Correlation_examples.png" alt="aaaa" /></p>

<center>图片取自wikipedia</center>


<p>由方差公式 Var(X+Y)=Var(X)+Var(Y)+Cov(X,Y)知道，如果 X，Y 为独立变量，则 Cov(X,Y)=0。但不能翻过来说。亦，如果 Cov(X,Y)=0， 则 X,Y 互为独立变量是错误的。比如在[-1,1]之间 $Cov(X,X<sup>2</sup>)$为0（根据 Cov 定义，代$入 X，X<sup>2</sup>积分可得），但不能说 X 和 X<sup>2</sup>$ 是独立的。</p>

<p>另一方面，如果两个随机变量非常相关，比如 Y=X，那么相关度公式</p>

<p>$$corr(X,X)=\frac{Var(X)}{\sqrt{Var(X)<sup>2</sup>}}=1$$</p>

<p>从这个特例可直觉上印证随机变量的相关度。</p>

<p>在线性代数中，对于一个矩阵 A，其由 N 个列向量组成，每个列向量可以看做是一个随机变量的不同值构成，那么它的协方差矩阵为</p>

<p>$$\Sigma=E[(X-E[X])^{T}(X-E[X])]$$</p>

<p>协方差矩阵在这里描述的是任意两个向量之间的线性相关度（一般将 E(x)归一化成0）。在机器学习中得降维处理中有所应用。其目的在于寻找一组基使得排除一些不重要的维度成为可能。</p>
]]></content>
  </entry>
  
</feed>
